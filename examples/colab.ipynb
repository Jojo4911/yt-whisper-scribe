{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# YT-Whisper-Scribe — Exécution sur Google Colab\n\n",
        "Ce notebook prépare un environnement Colab (GPU, dépendances, ffmpeg), monte Google Drive pour persister les sorties, et exécute la CLI du projet.\n\n",
        "Résumé des étapes:\n",
        "- Activer le GPU dans Exécution > Modifier le type d'exécution > Accélérateur matériel: GPU.\n",
        "- Installer dépendances (torch si besoin, whisper, yt-dlp, ffmpeg).\n",
        "- Monter Google Drive et définir les chemins.\n",
        "- Cloner le dépôt (ou placer le dossier du projet dans Drive).\n",
        "- Lancer la transcription avec le GPU (CUDA).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "# Vérification GPU\n",
        "import torch\n",
        "print('CUDA dispo:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        print('GPU:', torch.cuda.get_device_name(0))\n",
        "    except Exception as e:\n",
        "        print('Nom GPU non disponible:', e)\n",
        "else:\n",
        "    print('Activez le GPU: Exécution > Modifier le type d'exécution > Accélérateur: GPU')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Dépendances\n",
        "# Colab inclut souvent torch avec CUDA. Si nécessaire, décommentez et adaptez la ligne torch ci-dessous.\n",
        "# !pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!apt-get -y update && apt-get -y install ffmpeg\n",
        "!pip install -U openai-whisper yt-dlp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "# Monte Google Drive pour persister sorties et caches\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vars"
      },
      "outputs": [],
      "source": [
        "# Chemins et variables\n",
        "from pathlib import Path\n",
        "PROJECT_DIR = Path('/content/drive/MyDrive/YT-Whisper-Scribe')  # Modifiez si besoin\n",
        "OUTPUT_DIR = PROJECT_DIR / 'data'  # Sorties persistées dans Drive\n",
        "# Option: URL du dépôt si vous souhaitez cloner depuis GitHub\n",
        "REPO_URL = ''  # ex: 'https://github.com/<user>/YT-Whisper-Scribe.git'\n",
        "print('PROJECT_DIR =', PROJECT_DIR)\n",
        "print('OUTPUT_DIR  =', OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "get-project"
      },
      "outputs": [],
      "source": [
        "# Récupération du projet\n",
        "import os, subprocess, sys\n",
        "PROJECT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "if not any(PROJECT_DIR.iterdir()) and REPO_URL:\n",
        "    # Clone le dépôt si le dossier est vide et qu'une URL est fournie\n",
        "    print('Clonage du dépôt...', REPO_URL)\n",
        "    subprocess.run(['git', 'clone', REPO_URL, str(PROJECT_DIR)], check=True)\n",
        "elif not any(PROJECT_DIR.iterdir()) and not REPO_URL:\n",
        "    print('Placez le dossier du projet dans', PROJECT_DIR, 'ou renseignez REPO_URL puis relancez cette cellule.')\n",
        "# Crée le dossier de sortie\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "# (Option) Cache modèles sur Drive pour éviter les re-téléchargements\n",
        "CACHE_BASE = PROJECT_DIR / '.cache'\n",
        "os.makedirs(CACHE_BASE, exist_ok=True)\n",
        "os.environ['XDG_CACHE_HOME'] = str(CACHE_BASE)\n",
        "os.environ['TORCH_HOME'] = str(CACHE_BASE / 'torch')\n",
        "print('Caches:', os.environ.get('XDG_CACHE_HOME'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd"
      },
      "outputs": [],
      "source": [
        "%cd $PROJECT_DIR\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run-examples"
      },
      "source": [
        "## Exécuter la transcription\n\n",
        "- Par défaut, le modèle est `turbo`, le device `cuda`, la langue `en`, le glossaire `SWOOD_Glossary.json`, et la sortie dans `data/`.\n",
        "- Remplacez `URL_YOUTUBE` par l'URL souhaitée.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-basic"
      },
      "outputs": [],
      "source": [
        "# Exemple basique\n",
        "!python scripts/transcribe.py \"URL_YOUTUBE\" --output_dir \"$OUTPUT_DIR\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-advanced"
      },
      "outputs": [],
      "source": [
        "# Exemple avancé avec vocabulaire personnalisé et sortie .txt\n",
        "# Placez un vocabulaire (UTF-8, 1 terme/ligne) dans Drive, ex: $PROJECT_DIR/examples/vocab_example.txt\n",
        "!python scripts/transcribe.py \"URL_YOUTUBE\" \\\n+  --output_dir \"$OUTPUT_DIR\" \\\n+  --output_format txt \\\n+  --vocab_file examples/vocab_example.txt\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

